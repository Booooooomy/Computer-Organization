**Program Optimization**

Program Optimization: 어떤 프로그램을 더 효과적으로, 더 빠르게, 또 메모리 등의 자원을 덜 사용하면서 실행되게 하는 작업.
  - 예전에는 특정 프로그램을 가장 빠르게 작동시키는 방법은 그 프로그램의 어셈블리 코드를 적는 것이었지만 지금은 아니다.

그러나 위의 여러가지 이유가 서로 상충하는 경우가 있으므로 그것도 생각해줘야 한다.
그리고 cost of complexity. optimization을 하면 보통 코드 자체가 엉망이 되곤 한다. 그래서 오히려 효율적인 코드 만든다고 시간이 오히려 더 걸리는 경우도 있다.

그래서 optimization이 괜찮은 행위인지를 확인하기 위해 우리는 measure performance를 해야 한다.

방법 0. 물론 big O notation이 중요하지만, 이건 가장 기초가 되며 프로그램의 효율성 여부를 가장 쉽게 평가하는 뼈대다. 그러나 실제 현장에서는 big O 가 같은 2개의 프로그램도 효율성의 차이가 나는 경우가 많다.

방법 1.1. size measured statically: 
$ size a.out 커맨드를 사용한다. 
이건 text size(read-only part: instruction, constants), data size(read-write part: initialized variables), bss size(read-write part: data starts 0) 이 3개를 리턴한다.
text size와 data size는 flash를 빼먹는다.

방법 1.2. size measured dynamically

방법 2.1. clock time:
$ size/usr/bin/gcc $ size/usr/bin/size
그야말로 시간이 얼마나 걸리는지?

***** 방법 2.2. CPU time: <-------------------------------------------- 수업에서 주로 다룰 내용이다.
$ time/a.out
clock(real)
user CPU - my program's CPU
system CPU - kernel CPU executed on behalf of my program

방법 3. I/O count: <------------------------------------------ big data, machine learning에서는 3, 4가 주로 중요해지는 추세.
                                                                                     
방법 4. Network latency(DNS latency): <---------------------------------------------↙


**방법 2.2. CPU time에 관한 deeper thoughts**
2 kinds of real CPU time performance: 

1. high throughput(operations/second)
2. Low latency(sec): 프로그램 실행 후 실행 완료까지 걸리는 시간.

위의 2개는 서로 상충하는 경우가 많다. 이 때 우리는 1에 더 가산점을 준다고 가정하자.

만약 gcc -O2를 사용하면 -O보다 훨 빠르지만 문제가 있다.
-> only safe(can't change meaning under any circumstances) optimizations are allowed
  ex) gcc -O2 -ffastmath: 
    if(x<y && x+1<y+1)을 수학적으로 계산하자면 당연히 맞는 부분이라 && 부분을 날려버릴 수 있지만 gcc는 오버플로우 등의 문제가 걱정되어서 날려버리지 않는다.
    이때 ffastmath를 쓰면 수학적으로 맞는 수식은 모두 날려버리고 빠르게 답을 내는 데에 주안점을 둔다.
    - why safe optimization is hard?
        debugging, aliasing,..., side effects in functions...
        ex) int foo(int *a, int *b) {*a=2; *b=3; return *a+*b;} 하면 a와 b가 같은 경우(alias) 땜시로 예상치 못한 코드가 나올 수 있다.
            물론 이 때는 restrict라는 키워드로 둘을 구분해줌으로써 alise를 막는다.


Optimization blocker를 만났을 때 Amdahl's law? overall speedup (old time)/(new time)
~~ Amdahl's law를 좀 깊게 공부하자.



compiler가 optimize하는 요소들

1. remove redundant statements
필요 없는 부분 삭제: ex x = 1, y = 2, return x; 의 y = 2 부분 삭제.
당연한 부분 삭제: ex if (1 < 2) then return 1; 의 if 절 삭제.

2. compact/simplify insturctions
c++에서 x = x+1; 는 compiler가 x++로 바꾼다. 메모리도 덜 쓰고, 시간도 덜 쓰고.

3. choose faster instructions
5*2 는 multiplication 대신 left shift 1회 해줌.

4. removing repeated computation
(a+b+5) + (b+c+6) 은 a + (2*b) + c + 11로 바꿔 사용.

5. unrolling a loop
jump instruction은 시간을 가장 많이 뺏어먹는 놈 중에 하나다. 그래서 프로그램의 진행에서 jump의 개수를 가능하면 줄이려 한다.
예를 들어 for i = 0 to 100 print 1; 은 for i = 0 to 20 print 1; print 1; print 1; print 1; print 1;
이런 식으로 for loop(정확히 말하면 jump instruction)의 횟수 자체를 줄이는 것.

6. Extract an unchanging condition
for each p in s
  if time == day then p = y;
  else if time == night then p = b;

이 경우에 if와 else if절의 조건이 변하지 않기 때문에
if time == day
  for each p in s, p = y;
else if time == night
  for each p in s, p = b;

이게 더 나은 거란다. 나는 for loop이 두 번 사용되니까 그냥 안 좋을 거라고 생각했는데, 보니까 O(n)은 같고 반면에 첫 번째 녀석은 비교 구문이 계속 나오는 반면 두 번째 녀석은 한 번만 나오니까 맞는 말이다.

